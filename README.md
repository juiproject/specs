# Overview

A portable, file-based requirements management toolkit built on bash and SQLite. It provides a CLI for creating, querying, and maintaining structured requirements within any project, and a set of Claude Code skills that let Claude interact with those requirements conversationally.

The goal is to keep requirements close to the code they describe - versioned in the same repository, editable from the terminal, and accessible to AI-assisted development workflows without requiring a server, database service, or external tooling beyond bash and sqlite3.

# Contents

```
specs/
├── README.md
├── src/
│   ├── req                ← CLI script (bash + sqlite3)
│   └── schema.sql         ← SQLite database schema
└── claude/
    └── skills/
        ├── req/           ← Skill for querying and managing requirements
        └── spec-writer/   ← Skill for extracting and writing requirements
```

## `src/req`

The main CLI. A single bash script that manages a local SQLite database of requirements. Supports adding, listing, filtering, editing, updating (via YAML round-trip), tagging, exporting, snapshotting, and restoring requirements. Requirements are identified by display IDs (e.g. `AUTH-001`, `BIZ-003`) and organised into modules and categories.

## `src/schema.sql`

The SQLite schema for the requirements database. Used by `req init` to create or migrate the database. Defines tables for requirements, acceptance criteria, tags, dependencies, and modules.

## `claude/skills/req/`

A Claude Code skill that maps natural-language queries to `req` CLI commands. When installed, Claude can list, show, search, edit, tag, and update requirements through conversation rather than manual CLI invocation.

## `claude/skills/spec-writer/`

A Claude Code skill for extracting system requirements from source code, documentation, or conversation. It produces structured YAML output following a defined schema (MoSCoW priorities, categorised IDs, testable acceptance criteria) and can synchronise extracted requirements with the database via the `req` CLI.

# Installation

## Prerequisites

- **bash** 3.2+ (preinstalled on macOS and Linux)
- **sqlite3** (preinstalled on macOS; `sudo apt install sqlite3` on Debian/Ubuntu)

## Setup

1. Copy the `src/` contents into a `specs/` directory at the root of your project:

```bash
mkdir -p specs
cp src/req specs/req
cp src/schema.sql specs/schema.sql
chmod +x specs/req
```

2. Copy the Claude Code skills into your project's `.claude/skills/` directory:

```bash
mkdir -p .claude/skills
cp -r claude/skills/req .claude/skills/req
cp -r claude/skills/spec-writer .claude/skills/spec-writer
```

3. Register the skills in your project's `.claude/CLAUDE.md` so Claude knows when to use them:

```markdown
## Skills

| Skill | When to use | Path |
|-------|-------------|------|
| Spec Writer | Extracting, writing, or reviewing system requirements from code | `.claude/skills/spec-writer/SKILL.md` |
| Requirements Manager | Querying, browsing, editing, or managing requirements in the database | `.claude/skills/req/SKILL.md` |
```

4. Initialise the database and add the working files to `.gitignore`:

```bash
specs/req init

# Add to .gitignore
echo "specs/requirements.db" >> .gitignore
echo "specs/requirements.db-wal" >> .gitignore
echo "specs/requirements.db-shm" >> .gitignore
```

5. Optionally, set up the pre-commit hook to auto-snapshot requirements before each commit:

```bash
mkdir -p .githooks
cat > .githooks/pre-commit << 'HOOK'
#!/usr/bin/env bash
if [[ -f specs/req && -f specs/requirements.db ]]; then
    specs/req snapshot
    git add specs/snapshots/
fi
HOOK
chmod +x .githooks/pre-commit
git config --local core.hooksPath .githooks
```

## Getting started

```bash
# Add requirements interactively
specs/req add

# Or import from a YAML file
specs/req import requirements.yaml

# List and filter
specs/req list --priority must
specs/req show AUTH-001

# Snapshot for version control
specs/req snapshot
git add specs/snapshots/
```

See the `specs/README.md` generated by `req init` in your project for the full command reference.

# Future direction

The toolkit is currently a working proof-of-concept focused on structured requirements. The following areas are under consideration for future development.

## Additional artefacts

Requirements alone capture what a system must do, but not how users interact with it or why. Planned artefact types include:

- **User stories** - narrative descriptions of user goals and workflows, linked to the requirements they satisfy. Stories provide context that acceptance criteria alone cannot convey.
- **Essential use cases** - technology-independent interaction sequences between actors and the system. These sit between high-level stories and detailed requirements, helping to identify missing requirements and validate completeness.
- **Traceability matrix** - cross-referencing requirements to artefacts (stories, use cases, test cases, source files) so that coverage gaps and orphaned implementations are visible at a glance.

## Server mode

The current architecture is deliberately local - a single bash script, a SQLite file, YAML snapshots in git. This works well for small teams and single-repo projects. For larger organisations or cross-team visibility, a server-based mode could offer:

- A shared requirements database accessible over HTTP
- Role-based access and audit logging
- Real-time collaboration without the snapshot/restore cycle
- Integration with issue trackers and CI pipelines

The YAML import/export format is designed to serve as the interchange format between the local CLI and any future server implementation, so migration would be non-destructive.

## Beyond bash

The bash + sqlite3 approach keeps dependencies minimal and works everywhere, but it has limitations - particularly around YAML parsing, error handling, and testability. A future rewrite in a compiled or scripted language (e.g. Go, Python) could provide:

- Proper YAML parsing (the current regex-based approach is fragile for edge cases)
- A plugin or extension model for custom artefact types
- Richer output formatting (colour, interactive selection)
- A test suite for the CLI itself